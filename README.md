# Orisun Research

**Decision Intelligence Infrastructure for AI-Native Organizations**

---

## The Core Question

> *"What would business infrastructure look like if we designed it today, with AI as a first-class primitive?"*

Not "add AI to existing processes." Not "automate what we already do." But: **What is the AI-native version of this?**

---

## The Gap

Most organizations approaching AI face a fundamental problem: they're retrofitting intelligence onto systems designed for a different era.

- **Workflow tools** manage tasks, deadlines, and status. They don't interpret authority or capture judgment.
- **GRC platforms** manage controls and evidence. They're rarely domain-native and heavy to implement.
- **AI assistants** generate text. They have no governance, no provenance, no structured refusal.
- **"AI agents"** automate sequences. They don't capture decisions. They don't preserve accountability.

The result: organizations get faster at doing the wrong things, with less visibility into why.

---

## The Thesis

**Decision intelligence is a durable, cross-context value category.**

AI-native systems are the only viable way to build and scale it with sufficient speed, adaptability, and leverage—without eroding human agency.

This requires infrastructure that:

1. **Captures decisions, not just outcomes** — The reasoning matters as much as the result
2. **Anchors to authority** — Law, regulation, policy, terms. Not vibes.
3. **Refuses when context is insufficient** — Uncertainty is surfaced, not hidden
4. **Produces provenance by default** — The audit trail is a byproduct of operation
5. **Preserves human accountability** — AI assists judgment; it never replaces it
6. **Projects, never extracts** — Data stays where it belongs; only structured signals flow

---

## What "AI-Native" Actually Means

The term is widely used and rarely defined. Here's what it means in this architecture:

| Principle | Implication |
|-----------|-------------|
| **Ontology-first, not schema-first** | Each layer has a defined domain ontology designed for reasoning, not just storage |
| **Event- and signal-driven** | Systems reason over changes, deltas, posture, trajectories—not just static state |
| **Decision-centric, not workflow-centric** | The primary unit of value is a decision, a judgment, a trade-off. Workflows exist only to support decisions |
| **Learning-capable by design** | Every layer can observe outcomes, update representations, and improve future reasoning—without redesign |
| **Human-in-the-loop as primitive** | AI augments sense-making, option generation, risk surfacing. It does not replace authority |

**One conceptual universe. Multiple context-specific ontologies. Clean translation boundaries.**

---

## The Automation ≠ Advancement Distinction

This is the critical framing most miss:

| Approach | What It Does | What It Produces |
|----------|--------------|------------------|
| **Automation** | Run today's process faster | Speed without insight |
| **AI-Assisted** | Add AI features to existing workflows | Marginal improvement, same structure |
| **AI-Native** | Redesign the process around judgment + evented truth + governance | Compounding capability |

The question that creates advancement isn't "How do I automate what I do now?"

It's: **"If this were being invented today with AI as a first-class primitive, what would the workflow be?"**

---

## Research Methodology

This repository contains structural research on decision intelligence infrastructure.

All organizations, data, decisions, and contexts are **synthetic** and constructed solely to explore structural properties of decision intelligence systems.

- No real companies, markets, strategies, or individuals are represented
- The purpose is to study **form**, not **content**
- Synthetic environments are a legitimate method in systems research precisely because real data would contaminate ethics, IP, and interpretation

What we reveal:
- What must be represented
- What breaks when it isn't
- What design requirements emerge

What we do not reveal:
- Orchestration logic
- Enforcement mechanics
- How learning accumulates
- How governance tightens
- Implementation details

Anyone could understand the problem from this work. Building the solution requires different knowledge.

---

## Repository Structure

```
/thesis
  ├── decision-intelligence-thesis.md    # Full thesis statement
  ├── ai-native-definition.md            # What the term actually means
  └── velocity-thesis.md                 # Why AI-native compounds faster

/landscape
  ├── market-audit.md                    # What exists, what's missing
  └── category-positioning.md            # Where this sits

/primitives
  ├── decision-primitive.md              # What a decision record must contain
  ├── authority-primitive.md             # How authority layers compose
  ├── context-primitive.md               # What context capture requires
  └── evidence-primitive.md              # What provenance looks like

/research
  ├── methodology.md                     # Research approach
  └── /synthetic-orgs                    # Structural case studies
      ├── org-alpha-stable/
      ├── org-beta-fluid/
      └── org-gamma-multi-authority/

/visualizations
  ├── architecture-layers.html           # Interactive layer diagram
  └── decision-landscape.html            # Market gap visualization

/foundations
  └── world-models-and-jepa.md           # Technical foundations: representation learning

/prior-work
  └── ai-studio-foundation.md            # October 2025 research (97-tool catalog, Flourish viz)
```

---

## Key Findings (Summary)

From our structural research:

1. **The gap is architectural, not feature-level.** No amount of AI features added to existing systems produces decision intelligence. The layer must be designed for it.

2. **Refusal is a feature, not a bug.** Systems that always produce answers are systems that sometimes lie. First-class refusal—knowing what you don't know—is essential for governance-grade use.

3. **Authority must be explicit.** Decisions anchored to "best practices" or "common sense" cannot survive audit. Authority hierarchies (statute → regulation → policy → terms → entity choice) must be traversable.

4. **Learning compounds only with structure.** Raw data accumulation doesn't produce intelligence. Structured decision records with provenance do.

5. **Human accountability cannot be retrofitted.** If the system doesn't preserve who decided what and why from the beginning, no amount of logging after the fact creates auditability.

6. **The path forward is world models, not pattern matching.** Decision traces and context graphs become training signal for representations that survive semantic drift. Infrastructure built today enables learning tomorrow.

---

## Who This Is For

This research is relevant to:

- **Founders** building in regulated or judgment-heavy domains
- **Operators** trying to make AI useful without losing control
- **Architects** designing systems that must survive scrutiny
- **Researchers** studying organizational intelligence and AI governance

If you're thinking about substrates, governance-grade AI, authority-anchored reasoning, or the future of advisory services, we'd like to connect.

---

## Contact

[Contact information]

---

## License

This research is published under [appropriate license]. 

The findings, frameworks, and methodology are shared for discussion and advancement of the field. Implementation details, proprietary schemas, and production systems are not included and remain confidential.

---

*"The gap only closes when people operate differently—and tools are the only thing that makes that possible."*
